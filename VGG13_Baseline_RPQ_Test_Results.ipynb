{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2050c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Epoch 1/3\n",
      "Cache_hits:16388\n",
      "Cache_misses:33612\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 95.12807659999999 s\n",
      "TOTAL RPQ TIME TAKEN in EACH EPOCH: 8.592884800114916 s\n",
      "Epoch 2/3\n",
      "Cache_hits:18333\n",
      "Cache_misses:31667\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 94.36951150000095 s\n",
      "TOTAL RPQ TIME TAKEN in EACH EPOCH: 8.869013600036851 s\n",
      "Epoch 3/3\n",
      "Cache_hits:15325\n",
      "Cache_misses:34675\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 106.71051230000012 s\n",
      "TOTAL RPQ TIME TAKEN in EACH EPOCH: 9.458714499987764 s\n",
      "Total time:296.20810040000106s\n",
      "CYCLES: 756811.6965220027*10e6\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "\n",
    "CLOCK_SPEED = 2555  #(MHz) (Average speed of my RTX 4060)\n",
    "\n",
    "# 1. Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Data Preprocessing: Convert to grayscale with 1 channel and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale with 1 channel\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# 3. Load CIFAR-10 Dataset\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#full_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for CIFAR-10 (filtered)\n",
    "train_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4. Define VGG13 Model from Scratch for 1 channel grayscale input\n",
    "class VGG13(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG13, self).__init__()\n",
    "\n",
    "        # Convolutional layers with max pooling\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 1 * 1, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10)  # 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = torch.relu(self.conv6(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv7(x))\n",
    "        x = torch.relu(self.conv8(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(-1, 512 * 1 * 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize VGG13 model\n",
    "model = VGG13().to(device)\n",
    "\n",
    "# 5. Criterion and Optimizer\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 6. RPQ Function\n",
    "def rpq(input_batch, rows, columns):\n",
    "    # Flatten each image in the batch\n",
    "    flattened_batch = input_batch.view(input_batch.size(0), -1)  # Shape: (batch_size, rows)\n",
    "    # Perform matrix multiplication in parallel for the batch\n",
    "    signature = torch.matmul(flattened_batch, random_rpq_matrix)  # Parallel dot product\n",
    "    # Quantization\n",
    "    signature_quantized = torch.where(signature < 0, torch.ones_like(signature), torch.zeros_like(signature))\n",
    "    return signature_quantized\n",
    "\n",
    "# 7. Training Loop with CUDA Timing\n",
    "sync = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    cache_hits = 0\n",
    "    cache_misses = 0\n",
    "    mcache = []\n",
    "    total_rpq = 0\n",
    "    random_rpq_matrix = torch.randn(1024, 20, device=device).uniform_(-1,1) # Move RPQ matrix to GPU #mean = 0 and var = 1\n",
    "    start = perf_counter()\n",
    "\n",
    "    # Forward pass\n",
    "    for input_images, _ in train_loader:\n",
    "        input_images = input_images.to(device)  # Move batch to GPU\n",
    "\n",
    "        # Start RPQ computation\n",
    "        start_rpq = perf_counter()\n",
    "        rpq_signature_output = rpq(input_images, 1024, 20)  # Parallel RPQ for the entire batch\n",
    "        torch.cuda.synchronize()  # Synchronize GPU threads after RPQ computation\n",
    "        end_rpq = perf_counter()\n",
    "\n",
    "        # Generate binary keys for the entire batch\n",
    "        binary_keys = [''.join(map(str, row.int().tolist())) for row in rpq_signature_output]\n",
    "        total_rpq += end_rpq - start_rpq\n",
    "\n",
    "        # Sequential cache mechanism\n",
    "        for binary_key in binary_keys:\n",
    "            if binary_key in mcache:\n",
    "                cache_hits += 1\n",
    "            else:\n",
    "                cache_misses += 1\n",
    "                model(input_images)  # Pass the batch or individual image here as needed\n",
    "                mcache.append(binary_key)\n",
    "\n",
    "    # Synchronize at the end of the loop\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    end = perf_counter()\n",
    "    sync += (end - start)\n",
    "\n",
    "    # Compute elapsed time\n",
    "    print(f\"Cache_hits:{cache_hits}\")\n",
    "    print(f\"Cache_misses:{cache_misses}\")\n",
    "    print(f\"TOTAL TIME TAKEN in EACH EPOCH: {end - start} s\")\n",
    "    print(f\"TOTAL RPQ TIME TAKEN in EACH EPOCH: {total_rpq} s\")\n",
    "\n",
    "print(f\"Total time:{sync}s\")\n",
    "print(f\"CYCLES: {sync * CLOCK_SPEED}*10e6\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9809aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Epoch 1/3\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 112.79539299999851 s\n",
      "Epoch 2/3\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 114.68966560000081 s\n",
      "Epoch 3/3\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 111.29759549999835 s\n",
      "Total time:338.78265409999767s\n",
      "CYCLES: 865589.681225494*10e6\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "\n",
    "CLOCK_SPEED = 2555 #(MHz) (Average speed of my RTX 4060)\n",
    "\n",
    "# 1. Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Data Preprocessing: Convert to grayscale with 1 channel and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale with 1 channel\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# 3. Load CIFAR-10 Dataset\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#full_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for CIFAR-10 (filtered)\n",
    "train_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4. Define VGG13 Model from Scratch for 1 channel grayscale input\n",
    "class VGG13(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG13, self).__init__()\n",
    "\n",
    "        # Convolutional layers with max pooling\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 1 * 1, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10)  # 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = torch.relu(self.conv6(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv7(x))\n",
    "        x = torch.relu(self.conv8(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(-1, 512 * 1 * 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize VGG13 model\n",
    "model = VGG13().to(device)\n",
    "\n",
    "# 6. RPQ Function\n",
    "def rpq(input_batch, rows, columns):\n",
    "    # Flatten each image in the batch\n",
    "    flattened_batch = input_batch.view(input_batch.size(0), -1)  # Shape: (batch_size, rows)\n",
    "    # Perform matrix multiplication in parallel for the batch\n",
    "    signature = torch.matmul(flattened_batch, random_rpq_matrix)  # Parallel dot product\n",
    "    # Quantization\n",
    "    signature_quantized = torch.where(signature < 0, torch.ones_like(signature), torch.zeros_like(signature))\n",
    "    return signature_quantized\n",
    "\n",
    "# 7. Training Loop with CUDA Timing\n",
    "sync = 0\n",
    "start = 0\n",
    "end = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    start = perf_counter()\n",
    "    # Forward pass\n",
    "    for input_images, _ in train_loader:\n",
    "        input_images = input_images.to(device)  # Move batch to GPU\n",
    "        model(input_images)  # Pass the batch or individual image here as needed\n",
    "    end = perf_counter()\n",
    "    \n",
    "    print(f\"TOTAL TIME TAKEN in EACH EPOCH: {end - start} s\")\n",
    "    sync += (end - start)\n",
    "print(f\"Total time:{sync}s\")\n",
    "print(f\"CYCLES: {sync * CLOCK_SPEED}*10e6\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511b849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
