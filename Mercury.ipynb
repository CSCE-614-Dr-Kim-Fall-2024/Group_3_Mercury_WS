{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff40c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Baseline System - Total Computations: 720812736\n",
      "WS-RPQ System - Total Computations: 720812736\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 2\n",
      "Baseline System - Total Computations: 1441625472\n",
      "WS-RPQ System - Total Computations: 1441625472\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 3\n",
      "Baseline System - Total Computations: 2162438208\n",
      "WS-RPQ System - Total Computations: 2162438208\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 4\n",
      "Baseline System - Total Computations: 2883250944\n",
      "WS-RPQ System - Total Computations: 2883250944\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 5\n",
      "Baseline System - Total Computations: 3604063680\n",
      "WS-RPQ System - Total Computations: 3604063680\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 6\n",
      "Baseline System - Total Computations: 4324876416\n",
      "WS-RPQ System - Total Computations: 4324876416\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 7\n",
      "Baseline System - Total Computations: 5045689152\n",
      "WS-RPQ System - Total Computations: 5045689152\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 8\n",
      "Baseline System - Total Computations: 5766501888\n",
      "WS-RPQ System - Total Computations: 5766501888\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 9\n",
      "Baseline System - Total Computations: 6487314624\n",
      "WS-RPQ System - Total Computations: 6487314624\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n",
      "Iteration 10\n",
      "Baseline System - Total Computations: 7208127360\n",
      "WS-RPQ System - Total Computations: 7208127360\n",
      "WS-RPQ System - Total Computations Skipped due to Caching: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained VGG16 model and set it for training\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.train()  # Set model to training mode\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.01)\n",
    "\n",
    "# Cache for WS-RPQ\n",
    "class MCACHE:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "\n",
    "    def lookup(self, signature):\n",
    "        return self.cache.get(signature)\n",
    "\n",
    "    def insert(self, signature, result):\n",
    "        self.cache[signature] = result\n",
    "\n",
    "# Function to generate a binary signature using RPQ\n",
    "def generate_signature(input_tensor, random_matrix):\n",
    "    input_vector = input_tensor.view(-1)  # Flatten input tensor to a vector\n",
    "    projected = torch.matmul(input_vector, random_matrix)\n",
    "    signature = tuple((projected > 0).int().tolist())\n",
    "    return signature\n",
    "\n",
    "# Initialize a sample input tensor to determine dimensions after initial layers\n",
    "sample_input = torch.randn(1, 3, 224, 224)  # Single RGB image of size 224x224\n",
    "initial_output = vgg16.features[:4](sample_input)  # Output from first few layers\n",
    "flattened_dim = initial_output.view(-1).shape[0]  # Determine flattened size\n",
    "\n",
    "# Initialize the random projection matrix for RPQ with compatible dimensions\n",
    "projection_dim = 512  # Dimension for the RPQ signature\n",
    "random_matrix = torch.randn(flattened_dim, projection_dim)  # Adjusted random matrix size\n",
    "\n",
    "# Initialize cache\n",
    "mcache = MCACHE()\n",
    "\n",
    "# Tracking computation counts\n",
    "baseline_computation_count = 0\n",
    "ws_rpq_computation_count = 0\n",
    "ws_rpq_skipped_computations = 0\n",
    "\n",
    "# Function to simulate computation counts for VGG16 (approximation)\n",
    "def compute_mac_count(model):\n",
    "    # Calculate MACs per layer\n",
    "    mac_count = 0\n",
    "    for layer in model.features:  # Iterate only over convolutional layers\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            output_size = 7 * 7  # Approximate VGG16 output size after last conv layer\n",
    "            mac_count += layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1] * output_size\n",
    "    return mac_count\n",
    "\n",
    "# Baseline computation count for full forward pass\n",
    "baseline_mac_count = compute_mac_count(vgg16)\n",
    "\n",
    "# Training iterations simulation\n",
    "iterations = 10  # Reduced number of iterations for brevity\n",
    "input_data = torch.randn(1, 3, 224, 224)  # Single RGB image of size 224x224\n",
    "target = torch.tensor([1])  # Target label as a 1D tensor with a class index\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Baseline System: Full computation in forward pass each time\n",
    "    optimizer.zero_grad()\n",
    "    output = vgg16(input_data)  # Use entire model, including fully connected layers\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    baseline_computation_count += baseline_mac_count  # Count computations for baseline\n",
    "\n",
    "    # WS-RPQ System: Check cache before computing forward pass\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate a signature based on the output of initial layers in the feature extractor\n",
    "    initial_output = vgg16.features[:4](input_data)  # Take first few layers for signature\n",
    "    input_signature = generate_signature(initial_output, random_matrix)\n",
    "    cached_output = mcache.lookup(input_signature)\n",
    "\n",
    "    if cached_output is not None:\n",
    "        # Cache hit, reuse result and skip forward computation\n",
    "        output = cached_output\n",
    "        ws_rpq_skipped_computations += baseline_mac_count  # Count skipped computations due to caching\n",
    "    else:\n",
    "        # Cache miss, full computation in forward pass\n",
    "        output = vgg16(input_data)\n",
    "        mcache.insert(input_signature, output)\n",
    "        ws_rpq_computation_count += baseline_mac_count  # Count MACs only when not cached\n",
    "    \n",
    "    # Complete backpropagation and update weights for WS-RPQ system\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print results after each iteration\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Baseline System - Total Computations: {baseline_computation_count}\")\n",
    "    print(f\"WS-RPQ System - Total Computations: {ws_rpq_computation_count}\")\n",
    "    print(f\"WS-RPQ System - Total Computations Skipped due to Caching: {ws_rpq_skipped_computations}\\n\")\n",
    "\n",
    "    # Simulate new input for the next iteration\n",
    "    input_data = torch.randn(1, 3, 224, 224)  # Change input to simulate different inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c048a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29615175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d599555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29edd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd861f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612828c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Baseline System - Total Computations: 720812736\n",
      "\n",
      "Iteration 2\n",
      "Baseline System - Total Computations: 1441625472\n",
      "\n",
      "Iteration 3\n",
      "Baseline System - Total Computations: 2162438208\n",
      "\n",
      "Iteration 4\n",
      "Baseline System - Total Computations: 2883250944\n",
      "\n",
      "Iteration 5\n",
      "Baseline System - Total Computations: 3604063680\n",
      "\n",
      "Iteration 6\n",
      "Baseline System - Total Computations: 4324876416\n",
      "\n",
      "Iteration 7\n",
      "Baseline System - Total Computations: 5045689152\n",
      "\n",
      "Iteration 8\n",
      "Baseline System - Total Computations: 5766501888\n",
      "\n",
      "Iteration 9\n",
      "Baseline System - Total Computations: 6487314624\n",
      "\n",
      "Iteration 10\n",
      "Baseline System - Total Computations: 7208127360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained VGG16 model and set it for training\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.train()  # Set model to training mode\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.01)\n",
    "\n",
    "# Tracking computation counts (MACs)\n",
    "baseline_computation_count = 0\n",
    "\n",
    "# Function to approximate MAC count for VGG16's convolutional layers\n",
    "def compute_mac_count(model):\n",
    "    mac_count = 0\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            output_size = 7 * 7  # Approximate VGG16 output size after last conv layer\n",
    "            mac_count += layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1] * output_size\n",
    "    return mac_count\n",
    "\n",
    "# Baseline computation count per full forward pass\n",
    "baseline_mac_count = compute_mac_count(vgg16)\n",
    "\n",
    "# Training iterations simulation\n",
    "iterations = 10\n",
    "input_data = torch.randn(1, 3, 224, 224)  # Single RGB image of size 224x224\n",
    "target = torch.tensor([1])  # Target label as a 1D tensor with a class index\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Baseline System: Full computation in forward pass each time\n",
    "    optimizer.zero_grad()\n",
    "    output = vgg16(input_data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update computation count\n",
    "    baseline_computation_count += baseline_mac_count\n",
    "\n",
    "    # Print results after each iteration\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Baseline System - Total Computations: {baseline_computation_count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e98a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16c348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b69053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64207ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8e0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111ccbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d4ec6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 102760448 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22436\\2000656698.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# Ensure batch is a sequence of tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mbaseline_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mmercury_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmercury_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22436\\2000656698.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vishw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 102760448 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# RPQ Implementation\n",
    "class RPQ(nn.Module):\n",
    "    def __init__(self, input_dim, proj_dim):\n",
    "        super(RPQ, self).__init__()\n",
    "        self.proj_matrix = nn.Parameter(torch.randn(input_dim, proj_dim))\n",
    "        self.quant_levels = 2  # Binary quantization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Random Projection\n",
    "        proj = torch.matmul(x, self.proj_matrix)\n",
    "        # Quantization\n",
    "        quantized = torch.sign(proj)\n",
    "        return quantized\n",
    "\n",
    "# Custom Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Baseline VGG16 Model\n",
    "class BaselineVGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineVGG16, self).__init__()\n",
    "        self.vgg = vgg16(pretrained=True)\n",
    "        self.fc = nn.Linear(1000, 1)  # Assuming binary classification for simplicity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Mercury VGG16 Model with RPQ and MCACHE\n",
    "class MercuryVGG16(nn.Module):\n",
    "    def __init__(self, proj_dim):\n",
    "        super(MercuryVGG16, self).__init__()\n",
    "        self.vgg = vgg16(pretrained=True)\n",
    "        self.rpq = RPQ(1000, proj_dim)\n",
    "        self.fc = nn.Linear(proj_dim, 1)\n",
    "        self.mcache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        signature = self.rpq(x)\n",
    "        signature_tuple = tuple(signature.view(-1).tolist())  # Convert to hashable type\n",
    "        if signature_tuple in self.mcache:\n",
    "            return self.mcache[signature_tuple]\n",
    "        else:\n",
    "            result = self.fc(signature)\n",
    "            self.mcache[signature_tuple] = result\n",
    "            return result\n",
    "\n",
    "# Example usage\n",
    "input_dim = 1000\n",
    "proj_dim = 100\n",
    "data = [torch.randn(3, 224, 224) for _ in range(1000)]  # Example image data\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = MyDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)  # Set num_workers to 0\n",
    "\n",
    "# Models\n",
    "baseline_model = BaselineVGG16()\n",
    "mercury_model = MercuryVGG16(proj_dim)\n",
    "\n",
    "# Processing data\n",
    "baseline_outputs = []\n",
    "mercury_outputs = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    # Ensure batch is a sequence of tensors\n",
    "    batch = torch.stack([item for item in batch])\n",
    "    baseline_outputs.append(baseline_model(batch))\n",
    "    mercury_outputs.append(mercury_model(batch))\n",
    "\n",
    "# Calculate computations skipped\n",
    "total_computations = len(data) * input_dim\n",
    "skipped_computations = len(mercury_model.mcache) * proj_dim\n",
    "print(f\"Total computations: {total_computations}\")\n",
    "print(f\"Skipped computations: {skipped_computations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b27ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vectors:\n",
      " tensor([[ 9.6785e-01,  5.5491e-01,  5.5497e-01,  1.6345e+00,  2.1755e-01,\n",
      "          5.7401e-01, -5.9625e-01,  3.8363e-01, -3.5030e-01,  7.2375e-01,\n",
      "         -1.5138e-01,  6.0988e-01,  1.7080e+00,  1.4348e-02, -2.8090e+00,\n",
      "          8.8090e-01,  2.2408e-01, -3.2965e-01,  7.3395e-02, -2.4874e+00,\n",
      "          1.8154e+00,  1.8836e+00,  1.0003e+00, -8.7520e-01,  3.6680e-01,\n",
      "          5.3246e-01, -1.1220e+00, -9.2499e-01, -7.6712e-01, -5.5280e-01,\n",
      "         -1.1257e+00, -7.3899e-02,  1.2441e+00,  1.1490e+00,  1.2418e+00,\n",
      "          6.2535e-01, -4.0941e-01, -1.1545e+00,  1.3722e+00, -1.2433e+00,\n",
      "          1.6865e-01, -1.4706e-01, -1.9251e-01,  1.2917e+00, -1.3703e-01,\n",
      "         -1.0265e+00,  6.5060e-01, -1.8871e-01, -1.2930e+00,  9.6918e-01,\n",
      "          1.2805e-01, -1.8162e+00,  8.4305e-01,  1.3501e+00,  2.9058e-01,\n",
      "         -2.5975e-02,  7.7666e-01, -7.0208e-01, -6.1886e-01,  6.9238e-01,\n",
      "         -1.5172e+00,  1.4811e+00, -9.9828e-01, -1.9762e+00, -2.7931e-01,\n",
      "          7.9241e-01, -7.8442e-02, -6.2136e-01, -2.1128e+00, -1.2931e-01,\n",
      "         -9.9695e-01, -1.1266e+00, -1.9868e+00, -1.0743e+00, -2.9684e-01,\n",
      "         -5.3362e-01, -9.2493e-01, -5.8620e-01,  1.7063e+00, -1.5500e+00,\n",
      "         -7.2797e-01, -7.8912e-01, -6.2746e-01, -1.7956e+00, -3.6723e-01,\n",
      "         -1.0659e-01,  1.1984e+00,  6.1184e-01, -1.1117e+00,  1.4357e-01,\n",
      "         -1.3980e+00, -2.4690e+00, -1.5216e+00,  2.5114e-03,  1.1296e+00,\n",
      "         -1.4336e+00, -1.9700e+00,  3.2143e-01,  9.7040e-01,  9.9357e-01],\n",
      "        [-2.9456e+00, -3.1516e-01, -1.1677e+00,  7.9719e-01,  3.5009e-02,\n",
      "          9.8161e-02,  4.3332e-01, -1.9906e-01,  2.1199e-01, -9.6735e-01,\n",
      "         -2.9016e-01, -4.0392e-01,  4.1570e-01,  1.1758e+00,  2.9819e-01,\n",
      "          6.6079e-01,  1.0682e+00, -1.1994e+00, -5.5738e-02, -3.6957e-01,\n",
      "          3.8770e-01,  2.2546e-01, -2.5411e-01,  1.6621e-01, -6.6131e-01,\n",
      "          8.2485e-01, -9.3016e-01,  1.1595e+00,  3.1590e-01,  5.3537e-01,\n",
      "         -1.1569e+00,  6.0708e-01, -3.5352e-01,  1.8595e+00,  1.6418e+00,\n",
      "          4.2920e-01, -8.8491e-02, -1.3313e+00,  2.3124e+00, -1.0767e+00,\n",
      "         -1.5168e-01, -2.1728e+00, -6.8722e-01, -1.2899e+00, -8.3706e-02,\n",
      "         -9.2302e-01, -5.1882e-01,  8.7986e-01,  5.4146e-01, -2.2048e+00,\n",
      "          1.9478e+00,  5.3211e-01, -8.6494e-01,  1.5022e+00,  1.1617e-01,\n",
      "         -4.5671e-03,  1.3480e+00, -1.3847e+00, -4.2577e-02,  4.9656e-01,\n",
      "         -5.4190e-01,  3.1601e-01,  7.8707e-01,  1.1308e+00, -6.2257e-01,\n",
      "         -2.8484e-01, -4.9807e-02,  4.6399e-01,  1.9460e-01,  1.4799e-01,\n",
      "         -1.2334e-01, -2.8301e-01, -1.1715e+00,  5.9846e-01, -4.0820e-01,\n",
      "         -3.4288e-01, -1.1530e+00,  2.7223e-01,  1.3788e-01,  3.0552e-01,\n",
      "         -1.2144e+00, -1.6421e-01,  7.6624e-01,  2.1230e+00, -7.9831e-01,\n",
      "          3.1917e-01,  7.5084e-01, -6.1911e-01, -3.8173e-01,  1.0383e+00,\n",
      "         -9.9682e-01,  1.6234e-01, -3.0549e+00, -5.8382e-01,  2.1984e-01,\n",
      "          9.9377e-01, -1.4634e+00, -3.6649e-01,  1.3500e+00, -3.2499e-01],\n",
      "        [-2.3955e-01, -1.5909e-01, -1.8762e+00, -1.8907e-01, -1.4847e+00,\n",
      "          3.9291e-01, -2.7657e-01,  6.4089e-01,  1.0737e+00,  1.4942e+00,\n",
      "          3.7973e-01,  6.6688e-01, -4.1301e-01,  9.9679e-01, -7.4241e-01,\n",
      "          1.0960e+00,  1.4688e+00, -1.5246e-01, -2.1238e-01,  1.6648e-01,\n",
      "         -5.8641e-01, -4.2905e-01, -1.3507e+00,  5.3935e-01,  4.6185e-01,\n",
      "          2.6130e-01,  6.3476e-01,  7.5274e-01, -1.8129e-01, -4.2113e-02,\n",
      "          1.4524e+00,  9.0577e-02,  1.1540e+00,  5.5983e-01,  9.9212e-01,\n",
      "         -1.6082e-01,  5.3699e-01,  2.2638e-01, -7.9720e-01,  2.1870e-01,\n",
      "         -5.6770e-01, -1.3091e-01,  9.8525e-01, -1.4070e-01,  1.4724e+00,\n",
      "         -1.6197e+00,  4.4447e-01,  1.2117e-01,  1.9874e+00,  6.2642e-01,\n",
      "         -1.4272e-01, -3.5300e+00,  3.6284e-01, -2.1907e-01,  1.7142e-01,\n",
      "          3.4360e-01,  9.1070e-01, -7.8698e-01, -4.3609e-01, -5.1221e-01,\n",
      "          2.7394e-01,  1.0028e+00,  1.5396e+00,  5.6576e-01,  4.4641e-01,\n",
      "          5.8064e-01, -6.3539e-01,  1.1046e+00, -6.8823e-01,  1.2401e+00,\n",
      "         -3.3215e-01, -8.8440e-01, -1.8980e+00, -2.2026e-01,  2.8419e+00,\n",
      "         -3.6612e-01, -9.2841e-02, -3.0797e-01,  7.3820e-01,  1.9515e+00,\n",
      "          6.2558e-01,  7.8732e-01, -1.7441e-01,  7.7883e-01, -3.0637e-01,\n",
      "          4.7746e-01, -9.5356e-01,  1.0194e-01,  1.9662e-01,  9.8695e-01,\n",
      "          9.7404e-01, -1.6669e+00, -1.6592e-01, -6.1853e-02,  2.4076e-01,\n",
      "          2.6974e-01,  8.8543e-01,  1.0265e+00,  9.8738e-01,  8.9581e-01],\n",
      "        [ 9.3670e-01,  7.0552e-01,  2.1085e+00, -5.3905e-01, -3.8893e-01,\n",
      "          5.0693e-01, -5.3760e-01,  2.1049e+00,  1.5369e+00, -9.7071e-01,\n",
      "         -1.9286e-01,  7.9474e-03, -1.7680e+00,  1.7567e+00,  6.6227e-01,\n",
      "          1.1716e+00,  1.1884e+00,  1.0218e+00, -1.8884e+00, -1.0291e-01,\n",
      "         -1.0734e+00, -4.8409e-01, -5.0379e-01, -1.9553e-01,  3.9530e-01,\n",
      "          8.1222e-01,  6.7676e-01,  4.9878e-01,  3.6179e-01, -1.9418e+00,\n",
      "          2.0288e+00,  6.3440e-02,  2.1746e-02,  1.4451e+00,  1.3286e+00,\n",
      "          5.6237e-01, -1.5458e+00,  9.4171e-01,  1.4606e+00, -1.3296e-01,\n",
      "          3.6580e-01, -1.8796e-01,  4.2785e-01, -2.1902e-01,  2.0772e-01,\n",
      "          9.6559e-03,  6.6723e-01,  1.4696e+00, -1.1363e+00,  3.9151e-01,\n",
      "         -6.1684e-01,  1.6816e+00,  3.8769e-01,  1.6811e+00,  8.9554e-01,\n",
      "          1.5842e+00, -7.4472e-01, -7.7280e-01, -3.1266e-01, -6.3091e-01,\n",
      "          6.9788e-01, -1.2260e+00,  1.8311e+00, -7.3582e-01,  2.1280e-01,\n",
      "          1.2326e-01,  5.3090e-02,  1.1525e+00,  1.1502e+00, -3.1781e-01,\n",
      "         -6.3558e-01,  7.0100e-01,  1.0051e+00, -1.7759e+00,  1.7694e+00,\n",
      "         -2.4696e-01,  3.9098e-01,  6.7710e-01,  7.7643e-02,  6.8795e-01,\n",
      "          3.0424e-02, -1.1383e+00,  2.2367e+00, -2.8756e-01, -3.0672e-01,\n",
      "         -6.7228e-01,  4.0367e-01, -3.2908e-01, -3.4028e-01, -4.0507e-01,\n",
      "          6.0933e-02,  1.0224e+00,  3.8930e-01, -1.1480e+00,  1.0206e+00,\n",
      "          7.0563e-03, -9.2056e-01, -3.0783e-01, -1.3454e+00,  2.2783e-01],\n",
      "        [ 2.0969e+00, -2.7180e-01, -1.2607e+00,  1.3784e-01,  1.5761e-01,\n",
      "          1.3722e-01,  4.0080e-01,  3.5823e-01,  1.1784e+00,  9.6916e-01,\n",
      "          1.1396e+00,  1.6622e+00,  3.6379e-01,  1.2824e-01,  4.0573e-01,\n",
      "          3.2827e-01,  8.4534e-01, -1.8917e+00,  5.3213e-01, -6.1992e-01,\n",
      "         -1.3241e+00,  3.5977e-01, -4.3238e-01, -1.6346e+00,  1.5106e+00,\n",
      "          4.7526e-01, -8.6346e-01,  4.0680e-02, -2.7107e-01,  3.5497e-01,\n",
      "          7.1034e-02, -1.9816e-01,  2.5678e-02, -1.6601e+00,  1.0462e+00,\n",
      "         -3.8299e-01,  8.4096e-01, -7.3601e-01, -4.8397e-01,  1.0640e+00,\n",
      "          3.9007e-01,  1.0421e+00,  1.7506e+00,  1.7340e+00, -2.1645e+00,\n",
      "          6.3288e-01, -1.4176e-01,  5.5611e-01,  8.1810e-01, -1.0733e+00,\n",
      "          1.2071e+00, -2.0196e-02, -5.0778e-01, -6.0006e-01, -5.8805e-01,\n",
      "          1.0080e+00, -2.8515e-01, -4.6706e-02,  5.8347e-01,  9.9305e-01,\n",
      "          1.6926e-01, -1.3113e+00, -9.1159e-01, -1.9701e+00, -1.9452e-01,\n",
      "          3.6036e-01,  2.2788e+00, -5.7591e-02,  1.1685e+00,  3.5568e-01,\n",
      "         -9.4532e-01,  3.2174e-01, -9.6063e-01,  4.7675e-01, -1.7490e+00,\n",
      "          1.2655e+00, -9.5253e-02,  2.8826e-01, -6.0562e-01,  4.7222e-01,\n",
      "          7.1197e-01, -4.5283e-01,  5.5890e-02, -5.2822e-02,  6.7713e-01,\n",
      "         -1.6654e+00, -4.4351e-01, -1.0389e-01, -1.5169e-01, -1.1080e+00,\n",
      "         -2.2533e-02,  1.2057e+00,  6.4865e-01,  2.8287e-01, -6.2256e-01,\n",
      "          4.9218e-01,  2.1678e-01, -1.4658e-01, -6.4400e-01, -1.6054e+00],\n",
      "        [-5.6593e-02, -9.0619e-01, -9.2169e-01,  6.7997e-01, -1.2094e+00,\n",
      "          1.3589e+00,  1.4310e-01,  7.6260e-01,  3.6691e-01,  7.1726e-01,\n",
      "         -1.5003e+00, -1.2965e+00,  1.6818e-01,  1.1667e+00,  1.6802e+00,\n",
      "          4.2931e-01, -2.9597e-02,  2.0520e+00, -9.8798e-02,  1.6232e-01,\n",
      "          9.4391e-01, -8.8214e-01,  1.0146e+00, -4.9677e-01,  1.2498e+00,\n",
      "         -1.2613e+00, -8.4296e-01,  6.8399e-02, -1.3723e+00, -2.4709e+00,\n",
      "          3.3883e-01, -6.9999e-01, -1.8479e-01, -1.2148e+00,  5.2020e-01,\n",
      "          1.4478e+00, -9.2551e-01, -5.0160e-01,  1.3852e-01, -4.4101e-01,\n",
      "          3.0123e-01, -1.2290e+00, -1.6362e+00, -1.9247e+00,  9.9510e-01,\n",
      "          3.1795e-01,  7.0242e-01, -1.6181e+00,  4.4140e-01, -3.4181e-01,\n",
      "          8.2407e-01,  2.0397e+00, -1.5115e+00, -1.6325e+00, -4.7689e-01,\n",
      "          2.9500e-01, -9.9365e-02,  1.4236e-02,  1.0365e-01, -1.0788e+00,\n",
      "          4.1819e-01,  1.5504e-01, -7.6480e-01,  1.6261e+00, -2.2501e-01,\n",
      "          3.4032e-01, -6.4671e-01,  6.9160e-01,  1.0722e+00,  1.0410e+00,\n",
      "          4.3470e-01,  6.4929e-01,  1.2920e+00, -1.1552e-01, -6.8833e-01,\n",
      "         -1.3939e+00,  9.8992e-01, -1.9178e+00,  1.3047e+00, -2.6665e-01,\n",
      "         -1.9211e+00,  3.6024e-01,  8.4326e-01,  1.0831e+00,  1.0386e+00,\n",
      "         -2.3334e+00,  6.7311e-01, -7.7332e-02,  1.6481e+00, -4.6236e-01,\n",
      "          6.4356e-01,  9.2162e-01,  1.4210e+00,  9.8892e-01,  1.8057e+00,\n",
      "          5.0592e-01, -2.2821e-01,  4.4467e-01,  1.8535e+00,  1.3102e+00],\n",
      "        [-2.6743e+00, -1.0197e-01,  1.2716e+00,  6.0448e-01, -1.1060e+00,\n",
      "          4.3520e-01,  1.1201e+00, -4.1509e-01, -7.7126e-01, -9.8503e-01,\n",
      "         -3.1200e-01, -1.3131e+00,  1.5650e+00,  2.2405e+00,  3.2748e-01,\n",
      "          3.8427e-01,  1.3399e+00,  7.3791e-01, -2.3977e-01, -2.2422e-02,\n",
      "          1.1860e+00,  1.5461e+00,  6.0812e-01,  1.0428e+00, -1.9227e-01,\n",
      "         -1.1268e-02, -1.2862e+00,  4.0398e-01, -5.0828e-02, -9.9953e-01,\n",
      "         -7.0543e-01, -4.6384e-02, -4.8441e-01, -9.6405e-01,  9.5122e-01,\n",
      "          8.8370e-01, -1.3450e+00, -1.4470e+00,  9.7298e-01,  6.9471e-01,\n",
      "          1.0746e+00, -7.8729e-01,  1.0738e+00,  1.6938e-01, -8.8438e-01,\n",
      "          4.9662e-01,  2.2890e-01, -3.0627e-02, -9.1342e-01, -1.6659e+00,\n",
      "          1.9296e-01, -7.5640e-01,  7.2287e-01,  4.2575e-01,  1.3293e+00,\n",
      "         -9.8370e-01, -2.1065e+00,  1.2550e+00,  6.6644e-01, -2.3646e-01,\n",
      "          1.3785e-01,  1.7455e+00,  7.2245e-01,  7.6435e-02,  7.0161e-01,\n",
      "         -1.4224e-01,  5.2965e-01, -5.8508e-01, -1.3627e+00,  2.7980e-01,\n",
      "          1.4480e+00, -1.5811e+00,  2.5091e-01, -4.3443e-01, -4.2788e-01,\n",
      "          2.7040e-01, -8.5712e-01,  1.2840e+00, -1.1128e-01, -1.4950e+00,\n",
      "          1.4680e-01,  2.9488e-01, -1.0997e+00, -8.0456e-01,  1.5304e+00,\n",
      "          1.0170e-02,  7.0033e-01, -1.9909e+00, -9.0711e-01, -1.0674e+00,\n",
      "          7.9313e-01,  9.6427e-01, -8.1225e-02, -7.7730e-01,  5.1213e-02,\n",
      "         -5.4400e-01, -8.0601e-01, -4.7510e-01,  5.8797e-01,  2.4916e+00],\n",
      "        [ 1.2529e-01,  7.4076e-01,  3.6971e-01,  1.0307e-01, -5.8597e-01,\n",
      "         -1.1120e+00, -4.8058e-01, -2.4873e-01,  5.2811e-01, -7.2463e-01,\n",
      "          1.2059e+00,  2.3501e-01, -1.2815e+00,  7.2795e-01, -1.0122e+00,\n",
      "          1.7449e-01,  1.2295e+00, -1.5305e+00,  9.9438e-01, -2.4349e+00,\n",
      "         -4.0332e-01, -6.0530e-01,  3.4927e-01, -3.8339e-01, -7.1948e-01,\n",
      "          1.6494e+00,  3.7589e-01, -8.9532e-01,  1.8535e+00,  2.1036e-01,\n",
      "         -3.0073e-01, -2.7158e-01,  1.2936e+00, -1.5450e+00, -5.6569e-01,\n",
      "         -1.8970e+00,  1.5440e+00,  1.2735e+00,  7.0235e-01,  1.4182e+00,\n",
      "          1.5299e+00,  3.8030e-01,  2.1649e-01, -5.8577e-02, -3.1089e-02,\n",
      "          8.3872e-01,  3.0638e-01, -1.1782e+00,  2.2698e+00, -1.8048e-01,\n",
      "          5.1986e-02, -2.8446e-02,  1.1434e+00,  1.7792e-01,  1.3147e+00,\n",
      "          1.8554e-01, -1.6649e-01, -3.2037e-01,  2.7307e+00,  1.0139e+00,\n",
      "          4.9460e-01,  1.7919e-01, -3.7895e-02,  5.4917e-01,  3.3777e-01,\n",
      "         -6.7803e-01,  1.8548e-01, -2.0360e-01, -1.9967e-01,  5.4631e-01,\n",
      "         -4.2582e-01, -3.2325e-01,  2.9398e-01,  1.3889e-01, -7.5765e-02,\n",
      "         -5.7401e-01, -1.1593e+00,  1.7071e+00, -5.7545e-02,  7.6387e-01,\n",
      "         -5.6950e-01, -2.4173e-02,  1.6085e+00, -1.0556e+00, -4.6371e-01,\n",
      "         -1.0335e+00,  2.6376e-01, -1.2066e+00, -8.3044e-01,  2.8864e-01,\n",
      "         -1.8217e+00,  1.9646e+00, -9.6430e-01, -6.2444e-01,  1.1427e-01,\n",
      "         -2.3023e-03,  1.0876e+00, -7.5186e-01,  9.8613e-02, -2.2200e-01],\n",
      "        [ 2.5143e+00,  2.9678e-01,  1.8280e+00,  2.6714e+00, -7.0272e-01,\n",
      "          8.0678e-01,  1.2534e-01,  7.0172e-01, -1.3437e+00,  3.1636e-01,\n",
      "         -2.9874e-01,  2.1972e+00,  1.6859e+00,  6.6082e-01,  1.6018e-01,\n",
      "         -1.6051e-01, -4.4843e-01,  9.5466e-01,  2.9886e-01,  1.1503e-01,\n",
      "         -1.7156e+00,  8.7712e-01, -1.2377e+00, -5.5881e-01, -3.4390e-01,\n",
      "         -1.5383e-02,  1.5118e-01, -6.5125e-01,  1.3035e+00,  1.6153e-01,\n",
      "         -2.3079e-01, -6.9421e-01,  4.8617e-01, -1.8180e+00,  2.0698e-01,\n",
      "          6.4752e-01, -3.2736e-01, -4.0463e-01,  3.0582e-01,  5.2704e-01,\n",
      "          3.5812e-02,  9.3198e-01, -2.0055e+00,  3.8789e-01,  1.2221e+00,\n",
      "         -1.2032e+00,  1.3142e+00, -5.2571e-01, -6.5669e-01,  8.2192e-01,\n",
      "         -1.2168e+00, -9.3439e-01, -7.0561e-01, -9.9376e-01, -2.0062e+00,\n",
      "          6.4286e-01, -8.0080e-01,  4.2029e-03, -1.1422e+00,  3.7045e-01,\n",
      "         -2.5121e-01, -1.0076e+00, -1.6382e+00,  1.3102e+00,  7.3164e-01,\n",
      "          1.2823e+00, -5.0888e-01, -9.1847e-01, -8.4456e-01,  1.7397e-01,\n",
      "          1.0555e-01, -9.3613e-01,  1.6906e+00,  5.9944e-01,  2.5419e-02,\n",
      "         -6.0271e-01, -1.8950e+00, -1.5251e+00, -1.4778e+00, -8.5422e-01,\n",
      "          1.0007e+00, -1.0449e+00,  1.2203e+00,  4.9878e-01,  1.4583e-03,\n",
      "          7.2700e-01,  1.9012e+00,  9.9178e-01, -1.4769e-01, -3.0088e-01,\n",
      "         -4.9902e-01, -1.0917e+00,  1.5658e+00,  5.8672e-01, -8.0689e-01,\n",
      "          1.2487e+00,  8.2214e-01, -1.5464e-02,  6.5349e-01,  5.6046e-01],\n",
      "        [-4.8103e-01,  1.0554e+00,  3.6241e-01, -7.0799e-01,  7.3602e-01,\n",
      "         -8.6673e-01,  1.5735e+00, -5.7817e-01,  2.0049e-01,  2.3061e+00,\n",
      "          1.4982e-01,  6.6201e-02,  3.7865e-01, -3.2101e-01,  5.3310e-01,\n",
      "         -2.8764e-01, -5.3222e-01, -1.0210e+00,  7.9594e-01,  8.2365e-01,\n",
      "          9.3010e-01,  6.5644e-02,  3.3834e-01, -3.3663e-01,  1.0645e+00,\n",
      "         -1.7771e-02, -7.4622e-01, -6.2286e-01, -1.1471e+00,  4.8447e-01,\n",
      "         -1.0945e+00, -2.2137e-01,  7.9847e-01,  2.5974e+00, -9.2421e-01,\n",
      "         -2.9390e-01,  4.9680e-02,  8.0323e-01, -6.2131e-02, -6.6272e-01,\n",
      "          1.2572e+00,  2.3932e+00, -2.1362e+00, -1.9010e-01, -7.9126e-01,\n",
      "          1.9317e+00, -2.4530e-01, -2.6574e+00,  2.0544e-01,  1.2649e+00,\n",
      "         -9.6762e-01, -8.3348e-01,  1.9394e+00, -2.0751e+00,  1.3869e-01,\n",
      "         -3.0152e-01, -4.0479e-01,  4.7329e-01,  3.9995e-01,  1.7593e+00,\n",
      "          1.7094e+00, -4.2733e-01,  9.6406e-01, -9.3092e-01,  1.0479e+00,\n",
      "         -7.0383e-01,  2.1644e-01,  1.4775e-01, -5.2634e-01,  3.1275e+00,\n",
      "         -9.8441e-01, -8.8811e-01, -7.1374e-01, -1.1742e+00, -3.7129e-01,\n",
      "          7.9731e-01, -1.2878e+00, -1.2607e+00,  1.4304e-01,  8.1571e-01,\n",
      "          4.8569e-02, -1.3282e+00,  1.1573e+00,  9.7919e-02,  1.9835e+00,\n",
      "          3.8005e-02, -1.3632e+00,  5.6968e-01,  6.8591e-01, -8.5739e-02,\n",
      "          2.0192e+00,  1.6773e+00, -1.3318e+00,  1.2261e+00,  1.9111e+00,\n",
      "          7.0530e-01, -1.1940e+00, -6.5602e-01, -2.6900e-01, -1.9189e+00]])\n",
      "\n",
      "RPQ Signatures:\n",
      " tensor([[15, 11,  7,  9,  9, 16, 12, 10, 13,  9, 13,  0,  3,  5, 10, 11, 14,  1,\n",
      "          6, 14],\n",
      "        [ 9, 11,  5,  8,  8, 11,  8,  5, 16,  0,  9,  7,  6, 11,  7,  3,  6,  7,\n",
      "         10,  7],\n",
      "        [ 2,  2,  3,  2,  0,  9,  7,  0, 15,  1,  7,  9,  8,  5,  5, 11,  5, 16,\n",
      "          3,  4],\n",
      "        [ 8,  0,  8, 11,  9,  8, 13,  8,  9, 16, 11,  9, 10, 11, 11,  7, 14,  6,\n",
      "         12,  0],\n",
      "        [ 9, 13,  4,  8,  6,  5, 10,  0,  4, 11,  7, 16, 12,  4, 13,  9,  9,  8,\n",
      "         12,  8],\n",
      "        [ 5,  7,  7,  9,  9, 11,  4, 12, 16,  5,  7,  6,  9, 11,  9,  1, 11,  8,\n",
      "          9,  0],\n",
      "        [14, 12,  7,  8, 16, 11,  7, 12,  8, 11,  9,  1,  6, 11, 14,  6, 15,  0,\n",
      "         13, 13],\n",
      "        [ 7, 12,  8, 16,  3,  9, 10,  7,  1, 15, 13, 13,  5,  0,  5,  0, 10,  4,\n",
      "         13,  8],\n",
      "        [12,  8,  4, 10,  9,  0,  6, 16,  8, 13,  5,  4,  7,  4, 15, 12, 15, 10,\n",
      "          5, 15],\n",
      "        [ 7,  4,  2, 10,  1,  4,  8,  1,  0, 10,  6,  7, 10,  0,  0, 12,  8,  7,\n",
      "         16,  4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class RPQ:\n",
    "    def __init__(self, input_dim, projection_dim, quant_bits):\n",
    "        \"\"\"\n",
    "        Initialize the RPQ module.\n",
    "        :param input_dim: Dimensionality of the input vectors.\n",
    "        :param projection_dim: Dimensionality after random projection.\n",
    "        :param quant_bits: Number of bits for quantization.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.projection_dim = projection_dim\n",
    "        self.quant_bits = quant_bits\n",
    "        \n",
    "        # Random projection matrix (Gaussian random matrix)\n",
    "        self.projection_matrix = torch.randn(projection_dim, input_dim)\n",
    "        \n",
    "    def random_projection(self, x):\n",
    "        \"\"\"\n",
    "        Perform random projection on the input vector.\n",
    "        :param x: Input tensor of shape (batch_size, input_dim).\n",
    "        :return: Projected tensor of shape (batch_size, projection_dim).\n",
    "        \"\"\"\n",
    "        return torch.matmul(x, self.projection_matrix.T)\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        \"\"\"\n",
    "        Quantize the projected vector into a bit sequence.\n",
    "        :param x: Projected tensor of shape (batch_size, projection_dim).\n",
    "        :return: Quantized bit sequence of shape (batch_size, projection_dim).\n",
    "        \"\"\"\n",
    "        # Normalize the projected values to the range [0, 1]\n",
    "        x_min = x.min(dim=1, keepdim=True)[0]\n",
    "        x_max = x.max(dim=1, keepdim=True)[0]\n",
    "        x_normalized = (x - x_min) / (x_max - x_min + 1e-8)\n",
    "        \n",
    "        # Quantize to a discrete set of values based on the number of bits\n",
    "        quantized_x = torch.floor(x_normalized * (2 ** self.quant_bits)).int()\n",
    "        \n",
    "        return quantized_x\n",
    "    \n",
    "    def compute_signature(self, x):\n",
    "        \"\"\"\n",
    "        Compute the RPQ signature for the input vector.\n",
    "        :param x: Input tensor of shape (batch_size, input_dim).\n",
    "        :return: Signature tensor of shape (batch_size, projection_dim).\n",
    "        \"\"\"\n",
    "        projected_x = self.random_projection(x)\n",
    "        signature = self.quantize(projected_x)\n",
    "        \n",
    "        return signature\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define dimensions\n",
    "    input_dim = 100  # Input vector dimension\n",
    "    projection_dim = 20  # Projected dimension\n",
    "    quant_bits = 4  # Number of bits for quantization\n",
    "    \n",
    "    # Create an instance of RPQ\n",
    "    rpq = RPQ(input_dim=input_dim, projection_dim=projection_dim, quant_bits=quant_bits)\n",
    "    \n",
    "    # Generate random input vectors\n",
    "    batch_size = 10\n",
    "    inputs = torch.randn(batch_size, input_dim)\n",
    "    \n",
    "    # Compute RPQ signatures\n",
    "    signatures = rpq.compute_signature(inputs)\n",
    "    \n",
    "    print(\"Input Vectors:\\n\", inputs)\n",
    "    print(\"\\nRPQ Signatures:\\n\", signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896bc6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0: No similarity found. Performing computation.\n",
      "Computed Result: -10.730279922485352\n",
      "Input 1: No similarity found. Performing computation.\n",
      "Computed Result: -0.7254507541656494\n",
      "Input 2: No similarity found. Performing computation.\n",
      "Computed Result: -0.9253287315368652\n",
      "Input 3: No similarity found. Performing computation.\n",
      "Computed Result: -8.969884872436523\n",
      "Input 4: No similarity found. Performing computation.\n",
      "Computed Result: 2.577206611633301\n",
      "Input 5: No similarity found. Performing computation.\n",
      "Computed Result: 4.021418571472168\n",
      "Input 6: No similarity found. Performing computation.\n",
      "Computed Result: -5.820130348205566\n",
      "Input 7: No similarity found. Performing computation.\n",
      "Computed Result: -2.6905899047851562\n",
      "Input 8: No similarity found. Performing computation.\n",
      "Computed Result: -3.367391347885132\n",
      "Input 9: No similarity found. Performing computation.\n",
      "Computed Result: 9.375436782836914\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class RPQ:\n",
    "    def __init__(self, input_dim, projection_dim, quant_bits):\n",
    "        \"\"\"\n",
    "        Initialize the RPQ module.\n",
    "        :param input_dim: Dimensionality of the input vectors.\n",
    "        :param projection_dim: Dimensionality after random projection.\n",
    "        :param quant_bits: Number of bits for quantization.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.projection_dim = projection_dim\n",
    "        self.quant_bits = quant_bits\n",
    "        \n",
    "        # Random projection matrix (Gaussian random matrix)\n",
    "        self.projection_matrix = torch.randn(projection_dim, input_dim)\n",
    "        \n",
    "    def random_projection(self, x):\n",
    "        \"\"\"\n",
    "        Perform random projection on the input vector.\n",
    "        :param x: Input tensor of shape (batch_size, input_dim).\n",
    "        :return: Projected tensor of shape (batch_size, projection_dim).\n",
    "        \"\"\"\n",
    "        return torch.matmul(x, self.projection_matrix.T)\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        \"\"\"\n",
    "        Quantize the projected vector into a bit sequence.\n",
    "        :param x: Projected tensor of shape (batch_size, projection_dim).\n",
    "        :return: Quantized bit sequence of shape (batch_size, projection_dim).\n",
    "        \"\"\"\n",
    "        # Normalize the projected values to the range [0, 1]\n",
    "        x_min = x.min(dim=1, keepdim=True)[0]\n",
    "        x_max = x.max(dim=1, keepdim=True)[0]\n",
    "        x_normalized = (x - x_min) / (x_max - x_min + 1e-8)\n",
    "        \n",
    "        # Quantize to a discrete set of values based on the number of bits\n",
    "        quantized_x = torch.floor(x_normalized * (2 ** self.quant_bits)).int()\n",
    "        \n",
    "        return quantized_x\n",
    "    \n",
    "    def compute_signature(self, x):\n",
    "        \"\"\"\n",
    "        Compute the RPQ signature for the input vector.\n",
    "        :param x: Input tensor of shape (batch_size, input_dim).\n",
    "        :return: Signature tensor of shape (batch_size, projection_dim).\n",
    "        \"\"\"\n",
    "        projected_x = self.random_projection(x)\n",
    "        signature = self.quantize(projected_x)\n",
    "        \n",
    "        return signature\n",
    "\n",
    "# Cache mechanism to store and compare signatures\n",
    "class InputSimilarityCache:\n",
    "    def __init__(self):\n",
    "        # Dictionary to store signatures and their corresponding results\n",
    "        self.cache = {}\n",
    "    \n",
    "    def check_similarity(self, signature):\n",
    "        \"\"\"\n",
    "        Check if a given signature is already in the cache.\n",
    "        :param signature: The signature to check.\n",
    "        :return: Boolean indicating whether a similar signature exists in cache.\n",
    "                 If found, return True and the cached result; otherwise return False.\n",
    "        \"\"\"\n",
    "        sig_tuple = tuple(signature.view(-1).tolist())  # Convert tensor to tuple for hashing\n",
    "        \n",
    "        if sig_tuple in self.cache:\n",
    "            return True, self.cache[sig_tuple]\n",
    "        \n",
    "        return False, None\n",
    "    \n",
    "    def store_signature(self, signature, result):\n",
    "        \"\"\"\n",
    "        Store a new signature and its corresponding result in cache.\n",
    "        :param signature: The signature to store.\n",
    "        :param result: The result associated with this signature.\n",
    "        \"\"\"\n",
    "        sig_tuple = tuple(signature.view(-1).tolist())  # Convert tensor to tuple for hashing\n",
    "        self.cache[sig_tuple] = result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define dimensions\n",
    "    input_dim = 100  # Input vector dimension\n",
    "    projection_dim = 20  # Projected dimension\n",
    "    quant_bits = 4  # Number of bits for quantization\n",
    "    \n",
    "    # Create an instance of RPQ and cache\n",
    "    rpq = RPQ(input_dim=input_dim, projection_dim=projection_dim, quant_bits=quant_bits)\n",
    "    cache = InputSimilarityCache()\n",
    "    \n",
    "    # Generate random input vectors\n",
    "    batch_size = 10\n",
    "    inputs = torch.randn(batch_size, input_dim)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        # Compute RPQ signature for each input vector\n",
    "        signature = rpq.compute_signature(inputs[i].unsqueeze(0))\n",
    "        \n",
    "        # Check if a similar input has been processed before\n",
    "        is_similar, cached_result = cache.check_similarity(signature)\n",
    "        \n",
    "        if is_similar:\n",
    "            print(f\"Input {i}: Similarity found! Reusing cached result.\")\n",
    "            print(f\"Cached Result: {cached_result}\")\n",
    "            \n",
    "            # Here you would reuse the cached result instead of recomputing it\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            print(f\"Input {i}: No similarity found. Performing computation.\")\n",
    "            \n",
    "            # Perform some computation (e.g., forward pass through a neural network)\n",
    "            result = inputs[i].sum()  # Example computation\n",
    "            \n",
    "            # Store the new result in cache along with its signature\n",
    "            cache.store_signature(signature, result)\n",
    "\n",
    "            print(f\"Computed Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73015bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0: No similarity found. Performing computation.\n",
      "Computed Result: -19.56744956970215\n",
      "Input 1: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 2: No similarity found. Performing computation.\n",
      "Computed Result: -19.66850471496582\n",
      "Input 3: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 4: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 5: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 6: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 7: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 8: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n",
      "Input 9: Similarity found! Reusing cached result.\n",
      "Cached Result: -19.56744956970215\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 100  # Input vector dimension\n",
    "projection_dim = 20  # Projected dimension\n",
    "quant_bits = 4  # Number of bits for quantization\n",
    "\n",
    "# Create an instance of RPQ and cache\n",
    "rpq = RPQ(input_dim=input_dim, projection_dim=projection_dim, quant_bits=quant_bits)\n",
    "cache = InputSimilarityCache()\n",
    "\n",
    "# Generate a base input vector\n",
    "base_input = torch.randn(1, input_dim)\n",
    "\n",
    "# Generate slightly perturbed versions of the base input\n",
    "batch_size = 10\n",
    "inputs = base_input + 0.01 * torch.randn(batch_size, input_dim)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Compute RPQ signature for each input vector\n",
    "    signature = rpq.compute_signature(inputs[i].unsqueeze(0))\n",
    "    \n",
    "    # Check if a similar input has been processed before\n",
    "    is_similar, cached_result = cache.check_similarity(signature)\n",
    "    \n",
    "    if is_similar:\n",
    "        print(f\"Input {i}: Similarity found! Reusing cached result.\")\n",
    "        print(f\"Cached Result: {cached_result}\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Input {i}: No similarity found. Performing computation.\")\n",
    "        \n",
    "        # Perform some computation (e.g., forward pass through a neural network)\n",
    "        result = inputs[i].sum()  # Example computation\n",
    "        \n",
    "        # Store the new result in cache along with its signature\n",
    "        cache.store_signature(signature, result)\n",
    "\n",
    "        print(f\"Computed Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c8046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
