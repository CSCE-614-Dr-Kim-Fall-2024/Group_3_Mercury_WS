{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a389de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Epoch 1/3\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 78.46002820000103 s\n",
      "Epoch 2/3\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 77.35477870000068 s\n",
      "Epoch 3/3\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 79.64199440000084 s\n",
      "Total time: 235.45680130000255s\n",
      "CYCLES: 601592.1273215065 * 10^6\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "\n",
    "CLOCK_SPEED = 2555  # (MHz) Average speed of my RTX 4060\n",
    "\n",
    "# 1. Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Data Preprocessing: Convert to grayscale with 1 channel and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale with 1 channel\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# 3. Load CIFAR-10 Dataset\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 4. Define AlexNet Model for 1-channel Grayscale Input\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # Grayscale input, (3x32x32 -> 64x32x32)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64x32x32 -> 64x16x16\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),  # 64x16x16 -> 192x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 192x16x16 -> 192x8x8\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # 192x8x8 -> 384x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 384x8x8 -> 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 256x8x8 -> 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 256x8x8 -> 256x4x4\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten to (batch_size, 256*4*4)\n",
    "            nn.Linear(256 * 4 * 4, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(2048, 10) # 10 classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize AlexNet model\n",
    "model = AlexNet().to(device)\n",
    "\n",
    "# 6. Training Loop with CUDA Timing\n",
    "sync = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    start = perf_counter()\n",
    "    \n",
    "    for input_images, _ in train_loader:\n",
    "        input_images = input_images.to(device)  # Move batch to GPU\n",
    "        model(input_images)  # Pass the batch or individual image here as needed\n",
    "    \n",
    "    end = perf_counter()\n",
    "    print(f\"TOTAL TIME TAKEN in EACH EPOCH: {end - start} s\")\n",
    "    sync += (end - start)\n",
    "\n",
    "print(f\"Total time: {sync}s\")\n",
    "print(f\"CYCLES: {sync * CLOCK_SPEED} * 10^6\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba51d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Epoch 1/3\n",
      "Cache_hits:14665\n",
      "Cache_misses:35335\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 68.60868320000009 s\n",
      "TOTAL RPQ TIME TAKEN in EACH EPOCH: 6.192312399689399 s\n",
      "Epoch 2/3\n",
      "Cache_hits:16928\n",
      "Cache_misses:33072\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 70.47877130000052 s\n",
      "TOTAL RPQ TIME TAKEN in EACH EPOCH: 7.229701400194244 s\n",
      "Epoch 3/3\n",
      "Cache_hits:15906\n",
      "Cache_misses:34094\n",
      "TOTAL TIME TAKEN in EACH EPOCH: 72.39386770000056 s\n",
      "TOTAL RPQ TIME TAKEN in EACH EPOCH: 7.259202599612763 s\n",
      "Total time: 211.48132220000116s\n",
      "CYCLES: 540334.778221003 * 10^6\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "\n",
    "CLOCK_SPEED = 2555  # (MHz) Average speed of my RTX 4060\n",
    "\n",
    "# 1. Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Data Preprocessing: Convert to grayscale with 1 channel and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale with 1 channel\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# 3. Load CIFAR-10 Dataset\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 4. Define AlexNet Model for 1-channel Grayscale Input\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # Grayscale input, (3x32x32 -> 64x32x32)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64x32x32 -> 64x16x16\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),  # 64x16x16 -> 192x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 192x16x16 -> 192x8x8\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # 192x8x8 -> 384x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 384x8x8 -> 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 256x8x8 -> 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 256x8x8 -> 256x4x4\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten to (batch_size, 256*4*4)\n",
    "            nn.Linear(256 * 4 * 4, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(2048, 10) # 10 classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize AlexNet model\n",
    "model = AlexNet().to(device)\n",
    "\n",
    "# 5. RPQ Function\n",
    "def rpq(input_batch, rows, columns):\n",
    "    # Flatten each image in the batch\n",
    "    flattened_batch = input_batch.view(input_batch.size(0), -1)  # Shape: (batch_size, rows)\n",
    "    # Perform matrix multiplication in parallel for the batch\n",
    "    signature = torch.matmul(flattened_batch, random_rpq_matrix)  # Parallel dot product\n",
    "    # Quantization\n",
    "    signature_quantized = torch.where(signature < 0, torch.ones_like(signature), torch.zeros_like(signature))\n",
    "    return signature_quantized\n",
    "\n",
    "# 6. Training Loop with CUDA Timing\n",
    "sync = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    cache_hits = 0\n",
    "    cache_misses = 0\n",
    "    mcache = []\n",
    "    total_rpq = 0\n",
    "    random_rpq_matrix = torch.randn(1024, 20, device=device).uniform_(-1,1) # Move RPQ matrix to GPU #mean = 0 and var = 1\n",
    "    start = perf_counter()\n",
    "    # Forward pass\n",
    "    for input_images, _ in train_loader:\n",
    "        input_images = input_images.to(device)  # Move batch to GPU\n",
    "\n",
    "        # Start RPQ computation\n",
    "        start_rpq = perf_counter()\n",
    "        rpq_signature_output = rpq(input_images, 1024, 20)  # Parallel RPQ for the entire batch\n",
    "        torch.cuda.synchronize()  # Synchronize GPU threads after RPQ computation\n",
    "        end_rpq = perf_counter()\n",
    "\n",
    "        # Generate binary keys for the entire batch\n",
    "        binary_keys = [''.join(map(str, row.int().tolist())) for row in rpq_signature_output]\n",
    "        total_rpq += end_rpq - start_rpq\n",
    "\n",
    "        # Sequential cache mechanism\n",
    "        for binary_key in binary_keys:\n",
    "            if binary_key in mcache:\n",
    "                cache_hits += 1\n",
    "            else:\n",
    "                cache_misses += 1\n",
    "                model(input_images)  # Pass the batch or individual image here as needed\n",
    "                mcache.append(binary_key)    \n",
    "    \n",
    "    #sycnrhonize cuda cycles\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    end = perf_counter()\n",
    "    sync += (end - start)\n",
    "    # Compute elapsed time\n",
    "    print(f\"Cache_hits:{cache_hits}\")\n",
    "    print(f\"Cache_misses:{cache_misses}\")\n",
    "    print(f\"TOTAL TIME TAKEN in EACH EPOCH: {end - start} s\")\n",
    "    print(f\"TOTAL RPQ TIME TAKEN in EACH EPOCH: {total_rpq} s\")\n",
    "\n",
    "print(f\"Total time: {sync}s\")\n",
    "print(f\"CYCLES: {sync * CLOCK_SPEED} * 10^6\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23334be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c7022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
